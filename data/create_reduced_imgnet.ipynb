{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import dareblopy as db\n",
    "from IPython.display import Image, display\n",
    "import PIL.Image\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "\n",
    "S_BASE_PATH = \"data/imagenet_images\"\n",
    "T_BASE_PATH = \"data/imagenet_red\"\n",
    "\n",
    "from_path = os.path.join(os.getcwd(), S_BASE_PATH)\n",
    "to_path = os.path.join(os.getcwd(), T_BASE_PATH)\n",
    "\n",
    "IS_DEBUG = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processImagesByRatio(ratio: int, src_path : str, tar_path : str, postfix : str):\n",
    "    src_path = os.path.join(src_path, postfix)\n",
    "\n",
    "    # index-class mapping file\n",
    "    filename = \"index-\" + postfix + \".txt\"\n",
    "    fileHandle = open(os.path.join(tar_path, filename), 'w')\n",
    "\n",
    "    if not os.path.isdir(tar_path):\n",
    "        os.mkdir(tar_path)\n",
    "    tar_path = os.path.join(tar_path, postfix)\n",
    "\n",
    "    if not os.path.isdir(src_path):\n",
    "        print(f\"No source path found for {src_path}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.isdir(tar_path):\n",
    "        print(f\"creating target path at {tar_path}\")\n",
    "        os.mkdir(tar_path)\n",
    "\n",
    "    val_cl_src_paths = []\n",
    "    classes = os.listdir(src_path)\n",
    "    for val_class in classes:\n",
    "        val_cl_src_paths.append(os.path.join(src_path, val_class))\n",
    "    \n",
    "    print(f\"val classes: {len(val_cl_src_paths)}\")\n",
    "    \n",
    "    # copy all images from set\n",
    "    index = 0\n",
    "    img_name = \"\"\n",
    "    for class_name, src_path in zip(classes, val_cl_src_paths):\n",
    "        class_img_paths = os.listdir(src_path)\n",
    "        total_amount = len(class_img_paths)\n",
    "        cp_amount = int(ratio * total_amount)\n",
    "\n",
    "        print(f\"Copy {cp_amount}/{total_amount} to {tar_path}\")\n",
    "\n",
    "        for i in range(0, cp_amount):\n",
    "            if not os.path.isdir(tar_path):\n",
    "                os.mkdir(tar_path)\n",
    "\n",
    "            file_type = class_img_paths[i].split(\".\")[1]\n",
    "            # remove any wired URL encoded parts from the filetype\n",
    "            if len(\"jpg\") < len(file_type): \n",
    "                file_type = file_type[:3]\n",
    "            img_name = f\"{index}.{file_type}\"\n",
    "\n",
    "            cp_from = os.path.join(src_path, class_img_paths[i])\n",
    "            cp_to = os.path.join(tar_path, img_name)\n",
    "            fileHandle.write(class_name + \"\\n\")\n",
    "            if IS_DEBUG:\n",
    "                print(f\"Copy: \\n --{cp_from} \\n\" \n",
    "                        + f\" ->{cp_to}\")\n",
    "            else:\n",
    "                shutil.copyfile(cp_from, cp_to)\n",
    "            \n",
    "            index = index + 1\n",
    "    fileHandle.close()\n",
    "    return fileHandle.name\n",
    "\n",
    "def getClassToIndexMapping(path: str):\n",
    "    print(path)\n",
    "    if not os.path.isfile(path):\n",
    "        raise Exception\n",
    "    mapping = []\n",
    "    file = open(path, 'r')\n",
    "    for line in file:\n",
    "        mapping.append(line.replace(\"\\n\", \"\"))\n",
    "    file.close()\n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import numpy as np\n",
    "import time\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "\n",
    "def transformTrainImage(img: Image) -> Image:\n",
    "    trans = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return trans(img)\n",
    "\n",
    "def transformValImage(img: Image) -> Image:\n",
    "    trans = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return trans(img)\n",
    "\n",
    "def transformAllImages(path, set_type: str) -> None:\n",
    "\n",
    "    if path is None or not os.path.isdir(path):\n",
    "        raise Exception\n",
    "    \n",
    "    if set_type == \"val\":\n",
    "        transform = transformValImage\n",
    "\n",
    "    elif set_type == \"train\":\n",
    "        transform = transformTrainImage\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    img_name_list = os.listdir(path)\n",
    "\n",
    "    for img_name in img_name_list:\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        with PIL.Image.open(img_path) as img:\n",
    "            if img.mode == 'L':\n",
    "                img = transforms.Grayscale(num_output_channels=3)(img)\n",
    "            img = transform(img)\n",
    "            img = transforms.ToPILImage()(img)\n",
    "            img.save(img_path)\n",
    "\n",
    "def generateDatasetZipArchive(base_path: str, files_path: str, prefix: str, set_type: str) -> None:    \n",
    "    filenames = os.listdir(files_path)\n",
    "    zipPath = os.path.join(base_path, prefix + set_type + '.zip')\n",
    "    if os.path.isfile(zipPath):\n",
    "        os.remove(zipPath)\n",
    "    \n",
    "    with zipfile.ZipFile(zipPath, mode='w') as zipArch:\n",
    "        for filename in filenames:\n",
    "            zipArch.write(os.path.join(files_path, filename), arcname=filename)\n",
    "\n",
    "def generateNewImageDataset(from_base: str, to_base: str, set_type: str) -> None:\n",
    "    \n",
    "    if not (set_type == 'val' or set_type == 'train'):\n",
    "        raise Exception(f'{set_type} is not supported')\n",
    "    \n",
    "    target_path = os.path.join(to_path, set_type)\n",
    "    # clean up to_path\n",
    "    if os.path.isdir(target_path):\n",
    "        if IS_DEBUG: \n",
    "            print(f\"Removing existing directory: {target_path}\")\n",
    "        shutil.rmtree(target_path)\n",
    "        os.remove(os.path.join(to_base, f\"index-{set_type}.txt\"))\n",
    "    \n",
    "    ratio = 1/8\n",
    "    processImagesByRatio(ratio, from_base, to_base, set_type)\n",
    "    transformAllImages(os.path.join(to_base, set_type), set_type)\n",
    "    generateDatasetZipArchive(to_base, os.path.join(to_base, set_type), 'index-', set_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZippedDataset(torch.utils.data.Dataset):\n",
    "    img_class_mapping = []\n",
    "    archive = None\n",
    "    class_to_label = []\n",
    "\n",
    "    def __init__(self, arch_path: str, index_path: str, transforms=transforms.ToTensor()):\n",
    "        super(ZippedDataset, self).__init__()\n",
    "        if transforms is None:\n",
    "            raise Exception(\"Transforms must be set at least to ToTensor() at the end\")\n",
    "        # load index\n",
    "        self.img_class_mapping = getClassToIndexMapping(index_path)\n",
    "        self.archive = db.open_zip_archive(arch_path)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "        self.class_to_label = list(set(self.img_class_mapping))\n",
    "        self.class_to_label.sort()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_class_mapping)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        img_data = self.archive.read_jpg_as_numpy(f'{index}.jpg')\n",
    "        img_data = self.transforms(img_data)\n",
    "        return (img_data, self.class_to_label.index(self.img_class_mapping[index]))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/home/alex/Documents/pythonProjects/anytimeDnn/data/imagenet_red/index-train.txt\n/home/alex/Documents/pythonProjects/anytimeDnn/data/imagenet_red/index-val.txt\n0-60: torch.Size([8, 3, 224, 224]) - tensor([36, 15, 16, 27, 24,  3, 20,  2]) - load time: 0.02040719985961914 sec\n1-60: torch.Size([8, 3, 224, 224]) - tensor([ 1,  5, 31, 19, 38, 26,  4, 22]) - load time: 0.017916440963745117 sec\n2-60: torch.Size([8, 3, 224, 224]) - tensor([10,  9, 37, 30,  1, 23, 37, 30]) - load time: 0.014299154281616211 sec\n3-60: torch.Size([8, 3, 224, 224]) - tensor([10, 23, 18, 33, 21, 18, 12, 19]) - load time: 0.011503934860229492 sec\n4-60: torch.Size([8, 3, 224, 224]) - tensor([14,  1, 12, 24,  5, 37, 37, 36]) - load time: 0.012126922607421875 sec\n5-60: torch.Size([8, 3, 224, 224]) - tensor([36, 20, 11, 31, 37,  5, 21,  0]) - load time: 0.011279582977294922 sec\n6-60: torch.Size([8, 3, 224, 224]) - tensor([16,  1, 11, 13, 35, 14, 10, 29]) - load time: 0.01688218116760254 sec\n7-60: torch.Size([8, 3, 224, 224]) - tensor([20, 37, 19, 11, 24, 20, 36,  4]) - load time: 0.014063358306884766 sec\n8-60: torch.Size([8, 3, 224, 224]) - tensor([36, 22,  1,  7, 14, 23, 28, 31]) - load time: 0.013576269149780273 sec\n9-60: torch.Size([8, 3, 224, 224]) - tensor([ 9, 37,  5, 23, 31, 38, 22, 29]) - load time: 0.010852813720703125 sec\n10-60: torch.Size([8, 3, 224, 224]) - tensor([ 5, 32,  4, 10, 30, 20,  6, 30]) - load time: 0.01017618179321289 sec\n11-60: torch.Size([8, 3, 224, 224]) - tensor([33, 37,  3,  9, 28, 25, 35, 26]) - load time: 0.010995626449584961 sec\n12-60: torch.Size([8, 3, 224, 224]) - tensor([24, 31, 12, 23, 27, 36, 13, 20]) - load time: 0.01134347915649414 sec\n13-60: torch.Size([8, 3, 224, 224]) - tensor([ 8, 13, 35,  6, 18, 12,  5, 25]) - load time: 0.010452508926391602 sec\n14-60: torch.Size([8, 3, 224, 224]) - tensor([13,  0, 22, 21, 31,  6,  1, 30]) - load time: 0.024895429611206055 sec\n15-60: torch.Size([8, 3, 224, 224]) - tensor([27,  7, 21, 23, 33, 39, 33, 35]) - load time: 0.02117753028869629 sec\n16-60: torch.Size([8, 3, 224, 224]) - tensor([19, 18, 38, 25,  4,  8,  7, 38]) - load time: 0.012446880340576172 sec\n17-60: torch.Size([8, 3, 224, 224]) - tensor([30, 12,  2, 25,  9, 11, 14, 30]) - load time: 0.012034893035888672 sec\n18-60: torch.Size([8, 3, 224, 224]) - tensor([ 6,  8, 26, 16, 24, 20, 23, 25]) - load time: 0.014238119125366211 sec\n19-60: torch.Size([8, 3, 224, 224]) - tensor([17, 32, 33, 22, 27,  5, 17, 13]) - load time: 0.023981332778930664 sec\n20-60: torch.Size([8, 3, 224, 224]) - tensor([38, 23, 11, 11, 39, 29, 23, 37]) - load time: 0.012851238250732422 sec\n21-60: torch.Size([8, 3, 224, 224]) - tensor([12, 18, 34,  0,  4, 36,  9, 10]) - load time: 0.011189460754394531 sec\n22-60: torch.Size([8, 3, 224, 224]) - tensor([39,  3,  1,  3, 35,  5, 32,  9]) - load time: 0.011256933212280273 sec\n23-60: torch.Size([8, 3, 224, 224]) - tensor([17, 36, 34, 26, 32,  9,  8,  9]) - load time: 0.013250589370727539 sec\n24-60: torch.Size([8, 3, 224, 224]) - tensor([24,  7, 25, 15, 33, 26, 27, 17]) - load time: 0.012232780456542969 sec\n25-60: torch.Size([8, 3, 224, 224]) - tensor([28, 32, 33, 12, 27, 12, 29, 34]) - load time: 0.013470888137817383 sec\n26-60: torch.Size([8, 3, 224, 224]) - tensor([12, 35, 34, 15, 26, 11,  6, 29]) - load time: 0.011450767517089844 sec\n27-60: torch.Size([8, 3, 224, 224]) - tensor([15, 34, 39, 17, 39, 28, 24, 14]) - load time: 0.01974654197692871 sec\n28-60: torch.Size([8, 3, 224, 224]) - tensor([36, 22, 27, 29, 13, 29, 39, 38]) - load time: 0.02244710922241211 sec\n29-60: torch.Size([8, 3, 224, 224]) - tensor([ 2, 21, 15,  0,  0,  6, 17,  5]) - load time: 0.018555879592895508 sec\n30-60: torch.Size([8, 3, 224, 224]) - tensor([31, 32, 19, 22, 34, 15, 31, 32]) - load time: 0.01177358627319336 sec\n31-60: torch.Size([8, 3, 224, 224]) - tensor([ 7,  7, 30, 15,  9, 17, 24, 24]) - load time: 0.014067888259887695 sec\n32-60: torch.Size([8, 3, 224, 224]) - tensor([16,  0, 20, 14,  5, 37, 17, 21]) - load time: 0.011710166931152344 sec\n33-60: torch.Size([8, 3, 224, 224]) - tensor([18,  0,  1, 10, 30, 21,  3,  6]) - load time: 0.013276338577270508 sec\n34-60: torch.Size([8, 3, 224, 224]) - tensor([13,  3, 39, 35, 10, 27,  6,  7]) - load time: 0.013629674911499023 sec\n35-60: torch.Size([8, 3, 224, 224]) - tensor([31,  3, 20,  6, 35, 36, 17,  5]) - load time: 0.01179814338684082 sec\n36-60: torch.Size([8, 3, 224, 224]) - tensor([39,  3, 16, 28,  0, 19, 13, 23]) - load time: 0.011140823364257812 sec\n37-60: torch.Size([8, 3, 224, 224]) - tensor([34, 38,  8, 28,  2,  8, 12, 14]) - load time: 0.021305084228515625 sec\n38-60: torch.Size([8, 3, 224, 224]) - tensor([ 8,  2, 18, 24, 31, 19,  2, 32]) - load time: 0.011374711990356445 sec\n39-60: torch.Size([8, 3, 224, 224]) - tensor([24, 39, 10,  7, 25, 28, 29,  0]) - load time: 0.012877225875854492 sec\n40-60: torch.Size([8, 3, 224, 224]) - tensor([16, 16, 30, 21, 38, 34, 20, 27]) - load time: 0.016837596893310547 sec\n41-60: torch.Size([8, 3, 224, 224]) - tensor([ 4, 14, 13, 31,  4, 21, 31,  4]) - load time: 0.017735958099365234 sec\n42-60: torch.Size([8, 3, 224, 224]) - tensor([29, 15, 15, 18, 38, 25, 10,  1]) - load time: 0.023978233337402344 sec\n43-60: torch.Size([8, 3, 224, 224]) - tensor([ 3,  2, 16, 25, 30,  0,  8,  8]) - load time: 0.028787851333618164 sec\n44-60: torch.Size([8, 3, 224, 224]) - tensor([33, 30, 27,  2,  3, 14, 34, 21]) - load time: 0.01181173324584961 sec\n45-60: torch.Size([8, 3, 224, 224]) - tensor([ 6, 16,  8, 17,  9, 21, 37, 36]) - load time: 0.015352725982666016 sec\n46-60: torch.Size([8, 3, 224, 224]) - tensor([35,  8, 16, 24, 22, 25, 17,  7]) - load time: 0.012813568115234375 sec\n47-60: torch.Size([8, 3, 224, 224]) - tensor([22, 27, 19, 19, 10,  9, 23, 26]) - load time: 0.011431217193603516 sec\n48-60: torch.Size([8, 3, 224, 224]) - tensor([35, 26, 25,  2, 21, 33, 38, 26]) - load time: 0.016355276107788086 sec\n49-60: torch.Size([8, 3, 224, 224]) - tensor([28,  3, 26, 37, 39,  7, 17,  4]) - load time: 0.012183666229248047 sec\n50-60: torch.Size([8, 3, 224, 224]) - tensor([33, 28, 19, 19, 11,  0, 20,  4]) - load time: 0.01089024543762207 sec\n51-60: torch.Size([8, 3, 224, 224]) - tensor([13, 11, 22, 18, 34, 13,  1, 32]) - load time: 0.011470317840576172 sec\n52-60: torch.Size([8, 3, 224, 224]) - tensor([15, 22,  8, 32, 38, 29, 35,  5]) - load time: 0.013715028762817383 sec\n53-60: torch.Size([8, 3, 224, 224]) - tensor([29, 33, 18,  9,  1,  7, 10, 20]) - load time: 0.012360095977783203 sec\n54-60: torch.Size([8, 3, 224, 224]) - tensor([ 2, 15, 32, 33, 29, 34, 28,  6]) - load time: 0.012231588363647461 sec\n55-60: torch.Size([8, 3, 224, 224]) - tensor([25,  6,  4, 15,  2, 35, 11,  3]) - load time: 0.012279987335205078 sec\n56-60: torch.Size([8, 3, 224, 224]) - tensor([39, 32, 18, 26, 18, 16, 28, 16]) - load time: 0.03180885314941406 sec\n57-60: torch.Size([8, 3, 224, 224]) - tensor([38, 12, 22, 39,  2, 34, 10, 12]) - load time: 0.021558761596679688 sec\n58-60: torch.Size([8, 3, 224, 224]) - tensor([11, 13, 27, 19, 14,  1,  0, 28]) - load time: 0.015970945358276367 sec\n59-60: torch.Size([1, 3, 224, 224]) - tensor([7]) - load time: 0.0029821395874023438 sec\nAvg load time 0.0018744428364469436 sec\n"
    }
   ],
   "source": [
    "import gc\n",
    "#val_idx_filename = os.path.join(T_BASE_PATH, \"index-val.txt\")\n",
    "#train_idx_filename = os.path.join(T_BASE_PATH, \"index-train.txt\")\n",
    "\n",
    "#val_mapping = getClassToIndexMapping(val_idx_filename)\n",
    "#train_mapping = getClassToIndexMapping(train_idx_filename)\n",
    "\n",
    "#print(len(val_mapping))\n",
    "#print(len(set(val_mapping)))\n",
    "#print(len(train_mapping))\n",
    "#print(len(set(train_mapping)))\n",
    "\n",
    "#val_arch_path = os.path.join(to_path, \"imagenet_red_val.zip\")\n",
    "\n",
    "#print(val_arch_path)\n",
    "#val_archive = db.open_zip_archive(val_arch_path)\n",
    "\n",
    "\n",
    "#filename = processImagesByRatio(1/8, from_path, to_path, \"val\")\n",
    "#filename = processImagesByRatio(1/8, from_path, to_path, \"train\")\n",
    "\n",
    "#generateNewImageDataset(from_path, to_path, 'train')\n",
    "\n",
    "def benchmarkDataloader()\n",
    "\n",
    "\n",
    "train_dataset = ZippedDataset(os.path.join(to_path, 'index-train.zip'), os.path.join(to_path, 'index-train.txt'))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "#print(len(train_dataset))\n",
    "#print(len(val_dataset))\n",
    "#print(len(train_loader))\n",
    "#print(len(loader))\n",
    "\n",
    "MAX = 1000\n",
    "\n",
    "val_dataset = ZippedDataset(os.path.join(to_path, 'index-val.zip'), os.path.join(to_path, 'index-val.txt'))\n",
    "loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "avg = 0.0\n",
    "start = time.time()\n",
    "for i, (img, label) in enumerate(loader):\n",
    "    if i == MAX:\n",
    "        break\n",
    "    else:\n",
    "        stop = time.time() - start\n",
    "        print(f\"{i}-{len(loader)}: {img.shape} - {label} - load time: {stop} sec\")\n",
    "        avg = avg + stop\n",
    "    start = time.time()\n",
    "\n",
    "print(f\"Avg load time {avg / len(val_dataset)} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " 0.01531982421875 sec\n384-552: torch.Size([8, 3, 224, 224]) - tensor([32,  8,  4,  8, 39, 22, 30, 28]) - load time: 0.016425609588623047 sec\n385-552: torch.Size([8, 3, 224, 224]) - tensor([32,  9, 34,  5, 19,  5, 25,  3]) - load time: 0.016783714294433594 sec\n386-552: torch.Size([8, 3, 224, 224]) - tensor([29,  3, 10, 23, 15,  7, 20, 30]) - load time: 0.04177689552307129 sec\n387-552: torch.Size([8, 3, 224, 224]) - tensor([33, 38, 18, 12, 10, 28, 17, 21]) - load time: 0.015385627746582031 sec\n388-552: torch.Size([8, 3, 224, 224]) - tensor([28, 19, 21, 33,  4, 27, 39, 29]) - load time: 0.0161740779876709 sec\n389-552: torch.Size([8, 3, 224, 224]) - tensor([ 0,  7, 13,  7, 32,  5, 18,  6]) - load time: 0.013994932174682617 sec\n390-552: torch.Size([8, 3, 224, 224]) - tensor([16, 24, 17, 25, 12, 34, 38, 38]) - load time: 0.015782833099365234 sec\n391-552: torch.Size([8, 3, 224, 224]) - tensor([39, 17, 24, 23,  4,  3, 26, 14]) - load time: 0.014388322830200195 sec\n392-552: torch.Size([8, 3, 224, 224]) - tensor([25, 20,  7, 30, 13, 35, 24, 35]) - load time: 0.014690399169921875 sec\n393-552: torch.Size([8, 3, 224, 224]) - tensor([20, 22, 13,  6, 22, 26, 14, 32]) - load time: 0.016507625579833984 sec\n394-552: torch.Size([8, 3, 224, 224]) - tensor([10,  1,  8, 20,  9, 17, 18, 17]) - load time: 0.013829946517944336 sec\n395-552: torch.Size([8, 3, 224, 224]) - tensor([29, 10,  0,  9,  2, 16, 30, 36]) - load time: 0.014856338500976562 sec\n396-552: torch.Size([8, 3, 224, 224]) - tensor([35, 23, 26, 24, 11, 39, 35, 17]) - load time: 0.015228033065795898 sec\n397-552: torch.Size([8, 3, 224, 224]) - tensor([13, 20,  5, 33, 36, 10,  0,  8]) - load time: 0.022470712661743164 sec\n398-552: torch.Size([8, 3, 224, 224]) - tensor([28, 25, 33, 29, 35,  6,  7, 26]) - load time: 0.01499319076538086 sec\n399-552: torch.Size([8, 3, 224, 224]) - tensor([ 1, 39, 28, 34, 29, 29,  2, 31]) - load time: 0.02707505226135254 sec\n400-552: torch.Size([8, 3, 224, 224]) - tensor([37,  6,  2, 39,  4,  3, 39,  1]) - load time: 0.021598100662231445 sec\n401-552: torch.Size([8, 3, 224, 224]) - tensor([29,  0, 35, 24, 21, 13, 29, 18]) - load time: 0.018108606338500977 sec\n402-552: torch.Size([8, 3, 224, 224]) - tensor([ 9, 16, 21, 37, 36,  8, 34, 16]) - load time: 0.016520977020263672 sec\n403-552: torch.Size([8, 3, 224, 224]) - tensor([ 5,  0,  7,  9, 35, 12, 25, 16]) - load time: 0.013128280639648438 sec\n404-552: torch.Size([8, 3, 224, 224]) - tensor([ 7,  7, 11, 30, 30,  6,  4, 25]) - load time: 0.014068841934204102 sec\n405-552: torch.Size([8, 3, 224, 224]) - tensor([14, 21, 11, 17, 15,  6, 16, 19]) - load time: 0.014943838119506836 sec\n406-552: torch.Size([8, 3, 224, 224]) - tensor([ 8,  1, 16, 18, 25,  4, 38, 36]) - load time: 0.02060532569885254 sec\n407-552: torch.Size([8, 3, 224, 224]) - tensor([28,  5, 10, 13, 35,  5, 27, 34]) - load time: 0.022420883178710938 sec\n408-552: torch.Size([8, 3, 224, 224]) - tensor([20, 25, 15, 25, 32,  6, 34, 33]) - load time: 0.015279769897460938 sec\n409-552: torch.Size([8, 3, 224, 224]) - tensor([30, 38, 28, 26, 25, 24, 22, 39]) - load time: 0.014984369277954102 sec\n410-552: torch.Size([8, 3, 224, 224]) - tensor([25, 29, 12, 11, 10, 38, 10, 13]) - load time: 0.01811385154724121 sec\n411-552: torch.Size([8, 3, 224, 224]) - tensor([34, 16,  2, 29,  4, 26, 35, 13]) - load time: 0.03186631202697754 sec\n412-552: torch.Size([8, 3, 224, 224]) - tensor([24, 25, 15, 38, 14,  6,  4,  0]) - load time: 0.019141435623168945 sec\n413-552: torch.Size([8, 3, 224, 224]) - tensor([ 5,  5, 20, 25, 35,  1, 18, 18]) - load time: 0.015421390533447266 sec\n414-552: torch.Size([8, 3, 224, 224]) - tensor([11, 18, 32,  3,  0, 31,  3, 38]) - load time: 0.017696380615234375 sec\n415-552: torch.Size([8, 3, 224, 224]) - tensor([18,  0,  3,  7,  1, 12,  4,  8]) - load time: 0.014749765396118164 sec\n416-552: torch.Size([8, 3, 224, 224]) - tensor([26,  4, 37, 14, 33, 32,  4, 30]) - load time: 0.013653278350830078 sec\n417-552: torch.Size([8, 3, 224, 224]) - tensor([22, 16, 17, 27, 32, 21, 16,  7]) - load time: 0.013903617858886719 sec\n418-552: torch.Size([8, 3, 224, 224]) - tensor([31,  3, 25, 19, 34, 31, 35, 30]) - load time: 0.015046119689941406 sec\n419-552: torch.Size([8, 3, 224, 224]) - tensor([19, 21, 26, 33, 36, 23, 12, 38]) - load time: 0.021482229232788086 sec\n420-552: torch.Size([8, 3, 224, 224]) - tensor([17, 26, 39, 39, 16, 18, 29, 32]) - load time: 0.014147043228149414 sec\n421-552: torch.Size([8, 3, 224, 224]) - tensor([20,  2,  0,  4,  2, 17, 16, 19]) - load time: 0.014914751052856445 sec\n422-552: torch.Size([8, 3, 224, 224]) - tensor([11, 34,  9, 21,  4, 31, 37, 27]) - load time: 0.015420913696289062 sec\n423-552: torch.Size([8, 3, 224, 224]) - tensor([27, 31, 30, 36, 33,  0, 30, 26]) - load time: 0.0174100399017334 sec\n424-552: torch.Size([8, 3, 224, 224]) - tensor([13, 17, 36,  2, 37, 11, 24, 15]) - load time: 0.03185629844665527 sec\n425-552: torch.Size([8, 3, 224, 224]) - tensor([18, 19, 27, 32, 15,  0,  3, 28]) - load time: 0.015175819396972656 sec\n426-552: torch.Size([8, 3, 224, 224]) - tensor([16, 22, 18,  4, 26,  8, 14, 30]) - load time: 0.015110254287719727 sec\n427-552: torch.Size([8, 3, 224, 224]) - tensor([39,  4, 23, 14, 17, 30, 21, 14]) - load time: 0.012848854064941406 sec\n428-552: torch.Size([8, 3, 224, 224]) - tensor([14, 15,  8, 35, 16, 23, 10, 19]) - load time: 0.014412164688110352 sec\n429-552: torch.Size([8, 3, 224, 224]) - tensor([ 1, 31, 26, 13,  6,  5, 24, 31]) - load time: 0.01502680778503418 sec\n430-552: torch.Size([8, 3, 224, 224]) - tensor([39, 34, 10, 14, 19, 30,  1, 33]) - load time: 0.012922525405883789 sec\n431-552: torch.Size([8, 3, 224, 224]) - tensor([ 1, 20, 30,  2, 20, 30, 12, 25]) - load time: 0.02001333236694336 sec\n432-552: torch.Size([8, 3, 224, 224]) - tensor([32, 31, 24, 35, 34, 34,  6, 12]) - load time: 0.012169599533081055 sec\n433-552: torch.Size([8, 3, 224, 224]) - tensor([21, 29,  6,  8,  5,  5, 38, 24]) - load time: 0.013096094131469727 sec\n434-552: torch.Size([8, 3, 224, 224]) - tensor([27,  0, 20, 15, 17, 31,  6,  1]) - load time: 0.015245914459228516 sec\n435-552: torch.Size([8, 3, 224, 224]) - tensor([24, 16, 20, 19,  7, 34, 38, 32]) - load time: 0.015131950378417969 sec\n436-552: torch.Size([8, 3, 224, 224]) - tensor([22, 39,  6,  9, 24, 15, 19, 10]) - load time: 0.015416860580444336 sec\n437-552: torch.Size([8, 3, 224, 224]) - tensor([12, 17, 38,  0,  1, 32, 23,  1]) - load time: 0.024332761764526367 sec\n438-552: torch.Size([8, 3, 224, 224]) - tensor([19,  9, 26, 10, 15, 32,  1, 15]) - load time: 0.03383994102478027 sec\n439-552: torch.Size([8, 3, 224, 224]) - tensor([ 5, 30,  1,  2,  2, 15,  6, 10]) - load time: 0.0223538875579834 sec\n440-552: torch.Size([8, 3, 224, 224]) - tensor([26, 39,  9,  0, 23, 11, 20, 39]) - load time: 0.012004613876342773 sec\n441-552: torch.Size([8, 3, 224, 224]) - tensor([27, 17,  2, 24, 29, 18, 14, 11]) - load time: 0.014142274856567383 sec\n442-552: torch.Size([8, 3, 224, 224]) - tensor([12,  1,  6,  6, 19, 28, 31, 27]) - load time: 0.017769813537597656 sec\n443-552: torch.Size([8, 3, 224, 224]) - tensor([30, 18,  5, 25,  7, 22, 38, 17]) - load time: 0.023728609085083008 sec\n444-552: torch.Size([8, 3, 224, 224]) - tensor([ 2,  9, 21, 15, 28,  2, 17, 25]) - load time: 0.029162883758544922 sec\n445-552: torch.Size([8, 3, 224, 224]) - tensor([ 2, 31,  1, 38,  5, 17, 33, 28]) - load time: 0.019104719161987305 sec\n446-552: torch.Size([8, 3, 224, 224]) - tensor([38, 39, 21, 29, 12, 18, 36,  7]) - load time: 0.015326738357543945 sec\n447-552: torch.Size([8, 3, 224, 224]) - tensor([22,  1,  7, 36, 16, 39, 13, 25]) - load time: 0.025493144989013672 sec\n448-552: torch.Size([8, 3, 224, 224]) - tensor([ 8, 29,  0,  3, 28, 35, 29,  8]) - load time: 0.026144027709960938 sec\n449-552: torch.Size([8, 3, 224, 224]) - tensor([16, 23,  6, 22,  7,  2, 33,  8]) - load time: 0.023835420608520508 sec\n450-552: torch.Size([8, 3, 224, 224]) - tensor([12, 38, 31, 25,  6, 31, 37, 38]) - load time: 0.021723031997680664 sec\n451-552: torch.Size([8, 3, 224, 224]) - tensor([ 6,  7, 34, 16, 20, 28, 10,  3]) - load time: 0.015041351318359375 sec\n452-552: torch.Size([8, 3, 224, 224]) - tensor([ 4, 16, 28, 36, 37,  0, 12, 13]) - load time: 0.024855375289916992 sec\n453-552: torch.Size([8, 3, 224, 224]) - tensor([25, 15,  7, 20, 26, 28, 25,  3]) - load time: 0.011678457260131836 sec\n454-552: torch.Size([8, 3, 224, 224]) - tensor([15, 13,  0, 26, 24, 27, 34, 24]) - load time: 0.014348983764648438 sec\n455-552: torch.Size([8, 3, 224, 224]) - tensor([14, 16,  0, 20,  0, 33, 25, 13]) - load time: 0.013187408447265625 sec\n456-552: torch.Size([8, 3, 224, 224]) - tensor([39,  9, 29, 22, 33, 29, 36, 11]) - load time: 0.024842500686645508 sec\n457-552: torch.Size([8, 3, 224, 224]) - tensor([ 6, 37, 23, 11, 39, 25, 24, 31]) - load time: 0.01360464096069336 sec\n458-552: torch.Size([8, 3, 224, 224]) - tensor([ 6, 21, 27, 25,  0, 23, 21, 28]) - load time: 0.032087087631225586 sec\n459-552: torch.Size([8, 3, 224, 224]) - tensor([ 4, 35, 19, 34, 18, 10, 14, 32]) - load time: 0.015713214874267578 sec\n460-552: torch.Size([8, 3, 224, 224]) - tensor([ 2, 22, 32, 19, 39,  4,  0, 23]) - load time: 0.01310873031616211 sec\n461-552: torch.Size([8, 3, 224, 224]) - tensor([19,  9, 37, 10, 28, 13, 23,  8]) - load time: 0.021414518356323242 sec\n462-552: torch.Size([8, 3, 224, 224]) - tensor([33, 22, 37,  6, 34, 24, 31, 32]) - load time: 0.016733407974243164 sec\n463-552: torch.Size([8, 3, 224, 224]) - tensor([37, 18, 22, 23, 10, 25, 12, 18]) - load time: 0.012889385223388672 sec\n464-552: torch.Size([8, 3, 224, 224]) - tensor([23, 24, 13, 11,  7,  8, 15, 10]) - load time: 0.017605304718017578 sec\n465-552: torch.Size([8, 3, 224, 224]) - tensor([ 9, 38, 29,  6, 14,  7, 15,  1]) - load time: 0.014482498168945312 sec\n466-552: torch.Size([8, 3, 224, 224]) - tensor([16, 22, 10, 24, 21, 37,  5, 28]) - load time: 0.015387296676635742 sec\n467-552: torch.Size([8, 3, 224, 224]) - tensor([22,  3, 19, 28,  1,  9, 13, 34]) - load time: 0.014756917953491211 sec\n468-552: torch.Size([8, 3, 224, 224]) - tensor([35, 28,  6,  4, 18,  9, 19, 12]) - load time: 0.014750957489013672 sec\n469-552: torch.Size([8, 3, 224, 224]) - tensor([25,  3,  5,  9, 33, 20,  3, 36]) - load time: 0.016797780990600586 sec\n470-552: torch.Size([8, 3, 224, 224]) - tensor([31,  0, 31, 28, 28, 37, 11,  8]) - load time: 0.036314964294433594 sec\n471-552: torch.Size([8, 3, 224, 224]) - tensor([14, 24, 12, 31, 28,  4, 33,  3]) - load time: 0.023638248443603516 sec\n472-552: torch.Size([8, 3, 224, 224]) - tensor([18,  1, 16, 28, 34, 22, 21, 18]) - load time: 0.015425443649291992 sec\n473-552: torch.Size([8, 3, 224, 224]) - tensor([ 5, 14, 38, 18, 12, 32,  8, 19]) - load time: 0.02345418930053711 sec\n474-552: torch.Size([8, 3, 224, 224]) - tensor([24, 28, 10, 18, 35, 24, 21, 11]) - load time: 0.0154571533203125 sec\n475-552: torch.Size([8, 3, 224, 224]) - tensor([39,  1, 21,  7, 28, 27,  0, 24]) - load time: 0.01564621925354004 sec\n476-552: torch.Size([8, 3, 224, 224]) - tensor([18,  5, 19, 12, 32, 26,  1, 35]) - load time: 0.019891738891601562 sec\n477-552: torch.Size([8, 3, 224, 224]) - tensor([ 8, 15,  7,  1, 33, 30,  0, 21]) - load time: 0.023426294326782227 sec\n478-552: torch.Size([8, 3, 224, 224]) - tensor([ 6, 30, 26, 17, 11, 37, 19, 13]) - load time: 0.01695108413696289 sec\n479-552: torch.Size([8, 3, 224, 224]) - tensor([36, 29, 22, 15, 17, 29,  4,  4]) - load time: 0.014112710952758789 sec\n480-552: torch.Size([8, 3, 224, 224]) - tensor([24, 14,  6,  9, 31, 10,  5, 12]) - load time: 0.016135215759277344 sec\n481-552: torch.Size([8, 3, 224, 224]) - tensor([37, 21,  1, 29, 14, 13,  5,  0]) - load time: 0.03767657279968262 sec\n482-552: torch.Size([8, 3, 224, 224]) - tensor([ 2, 35, 31,  0, 18, 15, 22,  4]) - load time: 0.027028322219848633 sec\n483-552: torch.Size([8, 3, 224, 224]) - tensor([30, 38,  8, 30,  1, 33, 14,  9]) - load time: 0.016089916229248047 sec\n484-552: torch.Size([8, 3, 224, 224]) - tensor([31, 11, 24, 24, 13,  4, 36, 30]) - load time: 0.01930403709411621 sec\n485-552: torch.Size([8, 3, 224, 224]) - tensor([ 2, 16,  0, 33,  4, 14,  7,  6]) - load time: 0.015037059783935547 sec\n486-552: torch.Size([8, 3, 224, 224]) - tensor([21,  2, 21, 33, 27, 29, 24,  7]) - load time: 0.018746137619018555 sec\n487-552: torch.Size([8, 3, 224, 224]) - tensor([19, 25,  8, 13, 34, 29, 12,  5]) - load time: 0.018416881561279297 sec\n488-552: torch.Size([8, 3, 224, 224]) - tensor([19,  7,  1, 22, 12, 10, 30, 11]) - load time: 0.015247583389282227 sec\n489-552: torch.Size([8, 3, 224, 224]) - tensor([28, 36,  6, 14,  5,  6, 30, 35]) - load time: 0.015199422836303711 sec\n490-552: torch.Size([8, 3, 224, 224]) - tensor([17, 39, 19, 23, 33, 32, 39, 34]) - load time: 0.016006946563720703 sec\n491-552: torch.Size([8, 3, 224, 224]) - tensor([ 1, 33, 11,  7, 26, 27, 39, 29]) - load time: 0.014274120330810547 sec\n492-552: torch.Size([8, 3, 224, 224]) - tensor([ 4,  0, 32, 33, 30, 22, 11,  1]) - load time: 0.014039754867553711 sec\n493-552: torch.Size([8, 3, 224, 224]) - tensor([23, 32, 34, 28, 14, 21, 19, 25]) - load time: 0.027414798736572266 sec\n494-552: torch.Size([8, 3, 224, 224]) - tensor([ 0, 31, 34, 27, 19,  9, 32, 37]) - load time: 0.02355051040649414 sec\n495-552: torch.Size([8, 3, 224, 224]) - tensor([15, 23, 10, 38, 14, 20, 34,  3]) - load time: 0.016614913940429688 sec\n496-552: torch.Size([8, 3, 224, 224]) - tensor([12, 14,  4, 33, 26, 23,  6, 38]) - load time: 0.016878366470336914 sec\n497-552: torch.Size([8, 3, 224, 224]) - tensor([39, 38, 10, 28,  2,  0, 38, 32]) - load time: 0.01613140106201172 sec\n498-552: torch.Size([8, 3, 224, 224]) - tensor([ 5, 30, 21, 15,  8, 16, 38, 38]) - load time: 0.018253326416015625 sec\n499-552: torch.Size([8, 3, 224, 224]) - tensor([ 9,  3, 20,  5, 11, 36, 17, 20]) - load time: 0.014750957489013672 sec\n500-552: torch.Size([8, 3, 224, 224]) - tensor([22,  2, 23, 19,  2, 13, 32, 36]) - load time: 0.014888525009155273 sec\n501-552: torch.Size([8, 3, 224, 224]) - tensor([39, 29,  0,  1, 29,  1,  4, 20]) - load time: 0.016995668411254883 sec\n502-552: torch.Size([8, 3, 224, 224]) - tensor([ 2, 33, 37, 38,  7, 21, 16, 11]) - load time: 0.01706981658935547 sec\n503-552: torch.Size([8, 3, 224, 224]) - tensor([ 6, 21,  1, 37,  7, 35, 11,  6]) - load time: 0.02174234390258789 sec\n504-552: torch.Size([8, 3, 224, 224]) - tensor([22, 35, 12,  1,  5, 39, 38, 26]) - load time: 0.020265579223632812 sec\n505-552: torch.Size([8, 3, 224, 224]) - tensor([20, 21, 18, 16,  0, 38,  8, 35]) - load time: 0.03310275077819824 sec\n506-552: torch.Size([8, 3, 224, 224]) - tensor([35, 12, 17,  3, 19, 34,  6, 32]) - load time: 0.011093854904174805 sec\n507-552: torch.Size([8, 3, 224, 224]) - tensor([11,  9, 22, 39,  5, 27, 30,  0]) - load time: 0.01571345329284668 sec\n508-552: torch.Size([8, 3, 224, 224]) - tensor([21, 12, 19, 11, 23, 15, 22, 37]) - load time: 0.014890193939208984 sec\n509-552: torch.Size([8, 3, 224, 224]) - tensor([26, 28,  4, 39,  8, 20, 38, 28]) - load time: 0.021786212921142578 sec\n510-552: torch.Size([8, 3, 224, 224]) - tensor([37, 34, 12, 29, 24, 18, 18,  0]) - load time: 0.015239238739013672 sec\n511-552: torch.Size([8, 3, 224, 224]) - tensor([29, 10, 35,  6, 28, 37, 30, 29]) - load time: 0.016835689544677734 sec\n512-552: torch.Size([8, 3, 224, 224]) - tensor([32,  0,  3, 19, 28,  9,  2, 22]) - load time: 0.016537904739379883 sec\n513-552: torch.Size([8, 3, 224, 224]) - tensor([32, 20, 31, 24, 32, 35,  5, 10]) - load time: 0.015202760696411133 sec\n514-552: torch.Size([8, 3, 224, 224]) - tensor([30, 25, 37,  9, 37, 20,  8,  1]) - load time: 0.01633620262145996 sec\n515-552: torch.Size([8, 3, 224, 224]) - tensor([34, 16, 38, 21, 14, 29, 14, 24]) - load time: 0.012798547744750977 sec\n516-552: torch.Size([8, 3, 224, 224]) - tensor([34, 21, 31,  7, 36,  2, 13, 32]) - load time: 0.03510713577270508 sec\n517-552: torch.Size([8, 3, 224, 224]) - tensor([ 8,  6,  5, 25, 27, 36, 20,  1]) - load time: 0.019964933395385742 sec\n518-552: torch.Size([8, 3, 224, 224]) - tensor([24, 33,  5, 10, 37, 17, 11,  8]) - load time: 0.014717817306518555 sec\n519-552: torch.Size([8, 3, 224, 224]) - tensor([39, 25, 35,  0, 12,  5, 39, 10]) - load time: 0.012350082397460938 sec\n520-552: torch.Size([8, 3, 224, 224]) - tensor([11, 30,  2, 13, 31, 10, 26,  7]) - load time: 0.023433923721313477 sec\n521-552: torch.Size([8, 3, 224, 224]) - tensor([12,  1, 12,  7, 16, 13, 18, 11]) - load time: 0.014063119888305664 sec\n522-552: torch.Size([8, 3, 224, 224]) - tensor([ 1, 27, 32, 16, 27, 28, 10,  8]) - load time: 0.015388011932373047 sec\n523-552: torch.Size([8, 3, 224, 224]) - tensor([ 4, 10, 10,  9, 13, 32, 20, 24]) - load time: 0.01599907875061035 sec\n524-552: torch.Size([8, 3, 224, 224]) - tensor([25, 38,  0, 35, 18,  5, 27, 33]) - load time: 0.01596522331237793 sec\n525-552: torch.Size([8, 3, 224, 224]) - tensor([23,  9, 25, 38, 25,  3, 28, 24]) - load time: 0.012683391571044922 sec\n526-552: torch.Size([8, 3, 224, 224]) - tensor([26, 19, 32,  3, 31,  5, 32, 22]) - load time: 0.013953924179077148 sec\n527-552: torch.Size([8, 3, 224, 224]) - tensor([39, 15, 30, 27, 30, 30,  9, 31]) - load time: 0.016273021697998047 sec\n528-552: torch.Size([8, 3, 224, 224]) - tensor([31, 39, 38, 11, 22, 33, 12, 27]) - load time: 0.014376640319824219 sec\n529-552: torch.Size([8, 3, 224, 224]) - tensor([ 3,  2,  0, 33, 39,  3,  0,  4]) - load time: 0.03718829154968262 sec\n530-552: torch.Size([8, 3, 224, 224]) - tensor([29,  8, 30,  3, 27, 32, 14,  3]) - load time: 0.02320075035095215 sec\n531-552: torch.Size([8, 3, 224, 224]) - tensor([ 2, 23,  5,  0, 13, 32, 31, 24]) - load time: 0.015157461166381836 sec\n532-552: torch.Size([8, 3, 224, 224]) - tensor([11, 30, 31, 35,  3, 35, 16, 21]) - load time: 0.015507698059082031 sec\n533-552: torch.Size([8, 3, 224, 224]) - tensor([26,  6, 30, 15, 12,  6, 16,  8]) - load time: 0.015400409698486328 sec\n534-552: torch.Size([8, 3, 224, 224]) - tensor([10, 14, 35, 21, 30, 11, 19,  2]) - load time: 0.02045917510986328 sec\n535-552: torch.Size([8, 3, 224, 224]) - tensor([ 4, 38, 18,  2, 31, 20, 25, 28]) - load time: 0.014774799346923828 sec\n536-552: torch.Size([8, 3, 224, 224]) - tensor([ 4, 15, 17,  1, 20, 19, 32, 13]) - load time: 0.018279075622558594 sec\n537-552: torch.Size([8, 3, 224, 224]) - tensor([27, 38, 29, 22, 19, 38,  1, 16]) - load time: 0.013978958129882812 sec\n538-552: torch.Size([8, 3, 224, 224]) - tensor([ 7,  4, 37, 38, 35,  9, 22, 11]) - load time: 0.014470815658569336 sec\n539-552: torch.Size([8, 3, 224, 224]) - tensor([24,  5, 37,  3, 39, 16, 34, 13]) - load time: 0.015295982360839844 sec\n540-552: torch.Size([8, 3, 224, 224]) - tensor([ 9, 33, 14,  9, 17, 33, 38, 24]) - load time: 0.019838333129882812 sec\n541-552: torch.Size([8, 3, 224, 224]) - tensor([29, 12, 21, 23, 22, 27, 24, 30]) - load time: 0.03202223777770996 sec\n542-552: torch.Size([8, 3, 224, 224]) - tensor([36,  5, 20, 30,  9,  8, 13, 26]) - load time: 0.02675151824951172 sec\n543-552: torch.Size([8, 3, 224, 224]) - tensor([36,  2, 35, 38, 15, 31, 36, 39]) - load time: 0.012714624404907227 sec\n544-552: torch.Size([8, 3, 224, 224]) - tensor([15, 37, 22, 11, 38, 32, 10, 15]) - load time: 0.01630878448486328 sec\n545-552: torch.Size([8, 3, 224, 224]) - tensor([30, 23, 34, 29, 16, 11, 30, 26]) - load time: 0.013972043991088867 sec\n546-552: torch.Size([8, 3, 224, 224]) - tensor([31, 15, 22,  7, 34, 12,  8, 11]) - load time: 0.014786481857299805 sec\n547-552: torch.Size([8, 3, 224, 224]) - tensor([21,  0, 33, 27, 27, 18, 12, 12]) - load time: 0.014772653579711914 sec\n548-552: torch.Size([8, 3, 224, 224]) - tensor([12, 23, 20, 27,  9, 27,  4,  7]) - load time: 0.014362573623657227 sec\n549-552: torch.Size([8, 3, 224, 224]) - tensor([31, 10, 17,  3, 35, 25,  2,  1]) - load time: 0.016117334365844727 sec\n550-552: torch.Size([8, 3, 224, 224]) - tensor([12, 29, 26, 13,  0,  4, 26,  0]) - load time: 0.014312505722045898 sec\n551-552: torch.Size([5, 3, 224, 224]) - tensor([15, 16, 26, 32, 12]) - load time: 0.011385202407836914 sec\navg load time: 0.03693538551667691 sec\n"
    }
   ],
   "source": [
    "gc.collect()\n",
    "avg = 0.0\n",
    "for i, (img, label) in enumerate(train_loader):\n",
    "    if i == MAX:\n",
    "        break\n",
    "    else:\n",
    "        stop = time.time() - start\n",
    "        #print(f\"{i}-{len(train_loader)}: {img.shape} - {label} - load time: {stop} sec\")\n",
    "        avg += stop\n",
    "    start = time.time()\n",
    "print(f\"avg load time: {avg / len(train_dataset)} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python36964bitvenvvenv1df2604a792f44cb9c7aec421a0cc869"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}