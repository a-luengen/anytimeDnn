{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "eb9f6afae542b8db896e4dcf3a851d3ffe665752691e690e4032ea643745791b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy anytimeDnn data\n",
    "#!cp -r drive/My\\ Drive/reducedAnytimeDnn/* .\n",
    "!mkdir data\n",
    "!mkdir data/imagenet_red\n",
    "!mkdir data/imagenet_full\n",
    "!cp -r drive/My\\ Drive/reducedAnytimeDnn/data/utils.py ./data/utils.py \n",
    "!cp -r drive/My\\ Drive/reducedAnytimeDnn/data/ImagenetDataset.py ./data/ImagenetDataset.py\n",
    "!cp -r drive/My\\ Drive/reducedAnytimeDnn/data/__init__.py ./data/__init__.py\n",
    "!cp drive/My\\ Drive/reducedAnytimeDnn/data/imagenet_red/index-* ./data/imagenet_red\n",
    "#!cp drive/My\\ Drive/reducedAnytimeDnn/data/imagenet_full/index-* ./data/imagenet_full\n",
    "!cp -r drive/My\\ Drive/reducedAnytimeDnn/densenet .\n",
    "!cp -r drive/My\\ Drive/reducedAnytimeDnn/msdnet .\n",
    "!cp -r drive/My\\ Drive/reducedAnytimeDnn/resnet .\n",
    "!cp drive/My\\ Drive/reducedAnytimeDnn/utils.py ./utils.py\n",
    "!cp drive/My\\ Drive/reducedAnytimeDnn/train.py ./train.py\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r drive/My\\ Drive/reducedAnytimeDnn/requirements.txt\n",
    "#!pip3 install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!conda install pytorch==1.5.0 torchvision==0.6.0 cudatoolkit=10.1 -c pytorch\n",
    "!nvidia-smi\n",
    "#!pip install numpy\n",
    "#!pip uninstall torch torchvision\n",
    "#!pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\n",
    "!pip install dareblopy==0.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import msdnet.models\n",
    "\n",
    "from utils import *\n",
    "from data.ImagenetDataset import get_zipped_dataloaders, REDUCED_SET_PATH, FULL_SET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_PATH = os.path.join(os.getcwd(), 'runs', datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "DATA_PATH = REDUCED_SET_PATH\n",
    "IS_DEBUG = True\n",
    "DEBUG_ITERATIONS = 10\n",
    "STAT_FREQUENCY = 100\n",
    "LEARNING_RATE = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "GPU_ID = None\n",
    "START_EPOCH = 0\n",
    "EPOCHS = 2\n",
    "CHECKPOINT_INTERVALL = 4 \n",
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "\n",
    "LOG_FLOAT_PRECISION = ':6.4f'\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "writer = SummaryWriter(RUN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGUEMTNS FOR THE MSD-Net Configurtaion and Training\n",
    "class Object(object):\n",
    "  pass\n",
    "\n",
    "args = None\n",
    "\n",
    "try: \n",
    "  args = arg_parser.parse_args()\n",
    "except:\n",
    "  args = Object()\n",
    "\n",
    "  growFactor = list(map(int, \"1-2-4-4\".split(\"-\")))\n",
    "  bnFactor = list(map(int, \"1-2-4-4\".split(\"-\")))\n",
    "\n",
    "  args_dict = {\n",
    "      'gpu': 'gpu:0',\n",
    "      'use_valid': True,\n",
    "      'data': 'ImageNet',\n",
    "      'save': os.path.join(os.getcwd(), 'save'),\n",
    "      'evalmode': None,\n",
    "      'epoch': START_EPOCH,\n",
    "      'epochs': EPOCHS,\n",
    "      'arch': 'msdnet',\n",
    "      'seed': 42,\n",
    "      'test_interval': 2,\n",
    "\n",
    "      'grFactor': growFactor,\n",
    "      'bnFactor': bnFactor,\n",
    "      'nBlocks': 1,\n",
    "      'nChannels': 32,\n",
    "      'nScales': len(growFactor),\n",
    "      'reduction': 0.5,\n",
    "      'bottleneck': True,\n",
    "      'prune': 'max',\n",
    "      'growthRate': 16,\n",
    "      'base': 4,\n",
    "      'step': 4,\n",
    "      'stepmode': 'even',\n",
    "      \n",
    "      'lr': LEARNING_RATE,\n",
    "      'lr_type': 'multistep',\n",
    "      'momentum': MOMENTUM,\n",
    "      'weight_decay': WEIGHT_DECAY,\n",
    "      'resume': False,\n",
    "      'data_root': DATA_PATH,\n",
    "      'batch_size': BATCH_SIZE,\n",
    "      'workers': 1,\n",
    "      'print_freq': STAT_FREQUENCY\n",
    "  } \n",
    "\n",
    "  for key in args_dict:\n",
    "    setattr(args, key, args_dict[key])\n",
    "    #print(getattr(args, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    n_gpus_per_node = torch.cuda.device_count()\n",
    "    logging.info(f\"Found {n_gpus_per_node} GPU(-s)\")\n",
    "\n",
    "    # MAIN LOOP\n",
    "    model = msdnet.models.msdnet(args)\n",
    "\n",
    "    writer.add_graph(model, torch.rand(1, 3, 224, 224))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        logging.debug(\"Cuda is available.\")\n",
    "        logging.info(\"Using all available GPUs\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            logging.info(f\"gpu:{i} - {torch.cuda.get_device_name(i)}\")\n",
    "        model = nn.DataParallel(model).cuda()\n",
    "        logging.info(\"Moving criterion to device.\")\n",
    "        criterion = criterion.cuda()\n",
    "        cudnn.benchmark = True\n",
    "    else:\n",
    "        logging.info(\"Using slow CPU training.\")\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                    args.lr,\n",
    "                                    momentum=args.momentum,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "\n",
    "    calc_lr = lambda epoch: args.lr ** (1 + epoch // 30)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=calc_lr)\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_zipped_dataloaders(args.data_root, args.batch_size, use_valid=True)\n",
    "\n",
    "    best_prec1, best_epoch, start_epoch = 0.0, 0, 0\n",
    "\n",
    "    if hasattr(args, 'epoch') and args.epoch: # RESUME\n",
    "        model, optimizer, start_epoch, best_prec1  = resumeFromPath(\n",
    "            os.path.join(\n",
    "                os.getcwd(), \n",
    "                CHECKPOINT_DIR, \n",
    "                f\"msdnet_{args.epoch}{CHECKPOINT_POSTFIX}\"), \n",
    "            model, \n",
    "            optimizer)\n",
    "\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        logging.info(f\"Started Epoch{epoch + 1}/{EPOCHS}\")\n",
    "        # train()\n",
    "        train_loss, train_prec1, train_prec5, lr = train(train_loader, model, criterion, optimizer, scheduler, epoch)\n",
    "\n",
    "        writer.add_scalar('Train_CrossEntropy_Loss', train_loss / args.nBlocks, epoch)\n",
    "        writer.add_scalar('Train_prec1_avg', train_prec1, epoch)\n",
    "        writer.add_scalar('Train_prec5_avg', train_prec5, epoch)\n",
    "\n",
    "        # validate()\n",
    "        val_loss, val_prec1, val_prec5 = validate(val_loader, model, criterion)\n",
    "        scheduler.step()\n",
    "\n",
    "        is_best = val_prec1 > best_prec1\n",
    "        if is_best:\n",
    "            best_prec1 = val_prec1\n",
    "            best_epoch = epoch\n",
    "            logging.info(f'Best val_prec1 {best_prec1}')\n",
    "        \n",
    "        if is_best or epoch % CHECKPOINT_INTERVALL == 0:\n",
    "            state_dict = getStateDict(model, epoch, 'msdnet', best_prec1, optimizer)\n",
    "            save_checkpoint(state_dict, is_best, 'msdnet', CHECKPOINT_DIR)\n",
    "\n",
    "        if epoch % args.test_interval == 0:\n",
    "            logging.info(f'[{epoch}] - Running evaluation on test set.')\n",
    "            avg_loss, avg_top1, avg_top5 = validate(test_loader, model, criterion)\n",
    "            writer.add_scalar('test_loss', avg_loss, epoch + 1)\n",
    "            writer.add_scalar('test_top1', avg_top1, epoch + 1)\n",
    "            writer.add_scalar('test_top5', avg_top5, epoch + 1)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    logging.info(f'Best val_prec1: {best_prec1:.4f} at epoch {best_epoch}')\n",
    "\n",
    "    logging.info('*************** Final prediction results ***************')\n",
    "    validate(test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Batch Time', LOG_FLOAT_PRECISION)\n",
    "    losses = AverageMeter('Loss', LOG_FLOAT_PRECISION)\n",
    "    data_time = AverageMeter('Data Time', LOG_FLOAT_PRECISION)\n",
    "    top1, top5 = [], []\n",
    "    for i in range(args.nBlocks):\n",
    "        top1.append(AverageMeter(f'Top1-{i+1}', LOG_FLOAT_PRECISION))\n",
    "        top5.append(AverageMeter(f'Top5-{i+1}', LOG_FLOAT_PRECISION))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (img, target) in enumerate(val_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                img = img.cuda(non_blocking=True)\n",
    "                target = target.cuda(non_blocking=True)\n",
    "            \n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            output = model(img)\n",
    "            if not isinstance(output, list):\n",
    "                output = [output]\n",
    "\n",
    "            loss = 0.0\n",
    "            for j in range(len(output)):\n",
    "                loss += criterion(output[j], target)\n",
    "\n",
    "            losses.update(loss.item(), img.size(0))\n",
    "\n",
    "            for j in range(len(output)):\n",
    "                prec1, prec5 = accuracy(output[j].data, target, topk=(1,5))\n",
    "                top1[j].update(prec1.item(), img.size(0))\n",
    "                top5[j].update(prec5.item(), img.size(0))\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            \n",
    "            if i % args.print_freq == 0:\n",
    "                logging.info(f'Val - Epoch: [{i+1}/{len(val_loader)}]\\t'\n",
    "                      f'Time {batch_time.avg:.3f}\\t'\n",
    "                      f'Data {data_time.avg:.3f}\\t'\n",
    "                      f'Loss {losses.val:.4f}\\t'\n",
    "                      f'Acc@1 {top1[-1].val:.4f}\\t'\n",
    "                      f'Acc@5 {top5[-1].val:.4f}')\n",
    "            end = time.time()\n",
    "            \n",
    "            if IS_DEBUG and i == DEBUG_ITERATIONS:\n",
    "                return losses.avg, top1[-1].avg, top5[-1].avg\n",
    "\n",
    "    for j in range(args.nBlocks):\n",
    "        logging.info(f' * prec@1 {top1[j].avg:.3f} prec@5 {top5[j].avg:.3f}')\n",
    "    \n",
    "    return losses.avg, top1[-1].avg, top5[-1].avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, scheduler, epoch):\n",
    "    batch_time = AverageMeter('Batch Time', LOG_FLOAT_PRECISION)\n",
    "    data_time = AverageMeter('Data Time', LOG_FLOAT_PRECISION)\n",
    "    losses = AverageMeter('Loss', LOG_FLOAT_PRECISION)\n",
    "    top1, top5 = [],[]\n",
    "\n",
    "    for i in range(args.nBlocks):\n",
    "        top1.append(AverageMeter(f'Top1-{i+1}', LOG_FLOAT_PRECISION))\n",
    "        top5.append(AverageMeter(f'Top5-{i+1}', LOG_FLOAT_PRECISION))\n",
    "    \n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    running_lr = scheduler.get_last_lr()\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        image, target = data\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            image = image.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "                # time it takes to load data\n",
    "\n",
    "        output = model(image)\n",
    "        if not isinstance(output, list):\n",
    "            output = [output]\n",
    "\n",
    "        loss = 0.0\n",
    "        for j in range(len(output)):\n",
    "            loss += criterion(output[j], target)\n",
    "        \n",
    "        losses.update(loss.item(), image.size(0))\n",
    "\n",
    "        for j in range(len(output)):\n",
    "            prec1, prec5 = accuracy(output[j].data, target, topk=(1, 5))\n",
    "            top1[j].update(prec1.item(), image.size(0))\n",
    "            top5[j].update(prec5.item(), image.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            writer.add_scalar('training_loss', \n",
    "                                (losses.avg / len(output)) / args.print_freq,\n",
    "                                epoch * len(train_loader) + i)\n",
    "            logging.info(\n",
    "                f'Train - Epoch: [{epoch}][{i + 1}/{len(train_loader)}]\\t'\n",
    "                f'Time {batch_time.avg:.3f}\\t'\n",
    "                f'Data {data_time.avg:.3f}\\t'\n",
    "                f'Loss {losses.val:.4f}')\n",
    "\n",
    "            text = f'Train - Epoch: [{epoch}][{i + 1}/{len(train_loader)}] Acc@1: '\n",
    "            for j in range(len(output)):\n",
    "                text += f'c{j}={top1[j].avg:.4f} '\n",
    "            logging.info(text)\n",
    "            \n",
    "            text = f'Train - Epoch: [{epoch}][{i + 1}/{len(train_loader)}] Acc@5: '\n",
    "            for j in range(len(output)):\n",
    "                text += f'c{j}={top5[j].avg:.4f} '\n",
    "            logging.info(text)\n",
    "\n",
    "        if IS_DEBUG and i == DEBUG_ITERATIONS:\n",
    "            return losses.avg, top1[-1].avg, top5[-1].avg, running_lr\n",
    "\n",
    "    return losses.avg, top1[-1].avg, top5[-1].avg, running_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes accuracy over the k top predictions for the values of k\"\"\"\n",
    "    \n",
    "    # reduce memory consumption on following calculations\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        \n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-c963d26b939a776f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-c963d26b939a776f\");\n          const url = new URL(\"/\", window.location);\n          const port = 6007;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir \"{RUN_PATH}\" --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Found 0 GPU(-s)\n",
      "building network of steps: \n",
      "[4] 4\n",
      " ********************** Block 1  **********************\n",
      "|\t\tinScales 4 outScales 4 inChannels 32 outChannels 16\t\t|\n",
      "\n",
      "|\t\tinScales 4 outScales 3 inChannels 48 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 64, outChannels 32\t|\n",
      "\n",
      "|\t\tinScales 3 outScales 2 inChannels 32 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 48, outChannels 24\t|\n",
      "\n",
      "|\t\tinScales 2 outScales 1 inChannels 24 outChannels 16\t\t|\n",
      "|\t\tTransition layer inserted! (max), inChannels 40, outChannels 20\t|\n",
      "\n",
      "INFO:root:Using slow CPU training.\n",
      "INFO:root:Started Epoch1/2\n",
      "data/imagenet_red/index-train.txt\n",
      "data/imagenet_red/index-val.txt\n",
      "INFO:root:Train - Epoch: [0][1/497]\tTime 0.423\tData 0.061\tLoss 3.6633\tAcc@1 12.5000\tAcc@5 25.0000\n",
      "INFO:root:Train - Epoch: [0][1/497] Acc@1: c0=12.5000 \n",
      "INFO:root:Val - Epoch: [1/56]\tTime 0.195\tData 0.059\tLoss 3.7302\tAcc@1 0.0000\tAcc@5 0.0000\n",
      "INFO:root:Best val_prec1 1.1363636363636365\n",
      "DEBUG:root:checkpoints/msdnet_0_checkpoint.pth.tar\n",
      "INFO:root:[0] - Running evaluation on test set.\n",
      "INFO:root:Val - Epoch: [1/60]\tTime 0.078\tData 0.020\tLoss 3.6652\tAcc@1 0.0000\tAcc@5 0.0000\n",
      "INFO:root:Started Epoch2/2\n",
      "INFO:root:Train - Epoch: [1][1/497]\tTime 0.459\tData 0.059\tLoss 3.7712\tAcc@1 0.0000\tAcc@5 0.0000\n",
      "INFO:root:Train - Epoch: [1][1/497] Acc@1: c0=0.0000 \n",
      "INFO:root:Val - Epoch: [1/56]\tTime 0.194\tData 0.054\tLoss 3.6207\tAcc@1 0.0000\tAcc@5 12.5000\n",
      "INFO:root:Best val_prec1 2.272727272727273\n",
      "DEBUG:root:checkpoints/msdnet_1_checkpoint.pth.tar\n",
      "INFO:root:Best val_prec1: 2.2727 at epoch 1\n",
      "INFO:root:*************** Final prediction results ***************\n",
      "INFO:root:Val - Epoch: [1/60]\tTime 0.079\tData 0.019\tLoss 3.6468\tAcc@1 0.0000\tAcc@5 0.0000\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "try:\n",
    "  main(args)\n",
    "except Exception as e:\n",
    "  torch.cuda.empty_cache()\n",
    "  print(\"Oh no! Bad things happened...\")\n",
    "  print(e)\n",
    "  traceback.print_exc()\n",
    "finally:\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}