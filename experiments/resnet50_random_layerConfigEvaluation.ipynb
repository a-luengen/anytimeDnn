{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('capp': conda)",
   "metadata": {
    "interpreter": {
     "hash": "eb9f6afae542b8db896e4dcf3a851d3ffe665752691e690e4032ea643745791b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "import logging\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context import resnet\n",
    "from context import utils\n",
    "from context import data_loader\n",
    "from context import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATES CLASSIFICATION REPORT FOR \n",
    "\n",
    "BATCH_SIZE = 2\n",
    "STATE_DICT_PATH = '../state'\n",
    "DATA_PATH = '../data/imagenet_full'\n",
    "STORE_RESULT_PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(model, loader, classes, batch_size): \n",
    "    pred, grndT = [], []\n",
    "    for (images, labels) in iter(loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "            \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        pred = pred + [classes[predicted[k]] for k in range(min(batch_size, labels.shape[0]))]\n",
    "        grndT = grndT + [classes[labels[j]] for j in range(min(batch_size, labels.shape[0]))]\n",
    "    return grndT, pred\n",
    "\n",
    "def timed_decorator(function):\n",
    "    def inner(*args, **kwargs):\n",
    "        start = timer()\n",
    "        result = function(*args, **kwargs)\n",
    "        end = timer()\n",
    "        print(f'{(end - start) * 1000} [ms]')\n",
    "        return result\n",
    "    return inner\n",
    "\n",
    "@timed_decorator\n",
    "def executeQualityBench(arch_name: str, loader, skip_n: int, classes, batch_size: int):\n",
    "    model = utils.getModelWithOptimized(arch_name, n=skip_n, batch_size=batch_size)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "    model.eval()\n",
    "    arch = arch_name.split('-')[0]\n",
    "    model, _, _, _ = utils.resumeFromPath(os.path.join(STATE_DICT_PATH, f'{arch}_model_best.pth.tar'), model)\n",
    "    grndT, pred = evaluateModel(model, loader, classes, batch_size)\n",
    "\n",
    "    #logging.debug(metrics.classification_report(grndT, pred, digits=3))\n",
    "    #acc = metrics.accuracy_score(grndT, pred)\n",
    "    #prec = metrics.precision_score(grndT, pred, average='macro')\n",
    "    #rec = metrics.recall_score(grndT, pred, average='macro')\n",
    "    #f1 = metrics.f1_score(grndT, pred, average='macro')\n",
    "    return grndT, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../data/imagenet_red/index-val.txt\n",
      "../data/imagenet_red/index-train.txt\n",
      "../data/imagenet_red/index-val.txt\n"
     ]
    }
   ],
   "source": [
    "experiment_data = list()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "model_name = 'resnet50-drop-rand-n'\n",
    "\n",
    "skip_layers = 4\n",
    "classes = data_utils.getLabelToClassMapping(DATA_PATH)\n",
    "_, _, loader = data_loader.get_zipped_dataloaders(DATA_PATH, batch_size=BATCH_SIZE, use_valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = model_name.split('-')[0]\n",
    "print(f'Execute Non-Skip Run for {name}')\n",
    "model = utils.getModel(name)\n",
    "groundT, pred = executeQualityBench(name, loader, skip_n=0, classes=classes, batch_size=BATCH_SIZE)\n",
    "experiment_data.append({\n",
    "    'skip_n': 0,\n",
    "    'ground_truth': groundT,\n",
    "    'prediction': pred,\n",
    "    'configuration': [],\n",
    "    'acc': metrics.accuracy_score(groundT, pred)\n",
    "})\n",
    "\n",
    "for skip_layer in range(1, 7):\n",
    "    for run in range(0, 30):\n",
    "        print(f'Executing Run {run + 1} of 30')\n",
    "        model = utils.getModelWithOptimized(model_name, n=skip_layers, batch_size=BATCH_SIZE)\n",
    "        groundT, pred = executeQualityBench(model_name, loader, skip_n=skip_layers, classes=classes, batch_size=BATCH_SIZE)\n",
    "        policy = resnet.DropPolicies.getDropPolicy()\n",
    "        layer_config = []\n",
    "        if isinstance(policy, resnet.DropPolicies.ResNetDropRandNPolicy):\n",
    "            layer_config = policy.getSkipConfigurationList()\n",
    "        experiment_data.append({\n",
    "            'skip_n': skip_layers,\n",
    "            'ground_truth': groundT, \n",
    "            'prediction': pred, \n",
    "            'configuration': layer_config,\n",
    "            'acc': metrics.accuracy_score(groundT, pred)\n",
    "            })\n",
    "\n",
    "df = pn.DataFrame(experiment_data)\n",
    "df.to_csv(os.path.join(STORE_RESULT_PATH, 'resnet50_experiment_results.csv'), index=False)\n",
    "\n",
    "def print_data(data):\n",
    "    for datum in data:\n",
    "        print()\n",
    "        print(datum['skip_n'])\n",
    "        print(len(datum['ground_truth']))\n",
    "        print(len(datum['prediction']))\n",
    "        print(len(datum['configuration']))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   skip_n           ground_truth             prediction  \\\n",
       "0       1  [test2, test3, test3]  [test1, test2, test3]   \n",
       "1       3  [test1, test1, test1]  [test2, test2, test3]   \n",
       "2       1  [test3, test1, test3]  [test1, test1, test3]   \n",
       "\n",
       "                  configuration   acc  \n",
       "0    [True, False, False, True]  0.76  \n",
       "1     [True, True, False, True]  0.70  \n",
       "2  [False, False, False, False]  0.22  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>skip_n</th>\n      <th>ground_truth</th>\n      <th>prediction</th>\n      <th>configuration</th>\n      <th>acc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[test2, test3, test3]</td>\n      <td>[test1, test2, test3]</td>\n      <td>[True, False, False, True]</td>\n      <td>0.76</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>[test1, test1, test1]</td>\n      <td>[test2, test2, test3]</td>\n      <td>[True, True, False, True]</td>\n      <td>0.70</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>[test3, test1, test3]</td>\n      <td>[test1, test1, test3]</td>\n      <td>[False, False, False, False]</td>\n      <td>0.22</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "temp = list()\n",
    "\n",
    "temp.append({\n",
    "    'skip_n': 1,\n",
    "    'ground_truth': ['test2', 'test3', 'test3'], \n",
    "    'prediction': ['test1', 'test2', 'test3'], \n",
    "    'configuration': [True, False, False, True],\n",
    "    'acc': 0.76\n",
    "})\n",
    "temp.append({\n",
    "    'skip_n': 3,\n",
    "    'ground_truth': ['test1', 'test1', 'test1'], \n",
    "    'prediction': ['test2', 'test2', 'test3'], \n",
    "    'configuration': [True, True, False, True],\n",
    "    'acc': 0.70\n",
    "})\n",
    "temp.append({\n",
    "    'skip_n': 1,\n",
    "    'ground_truth': ['test3', 'test1', 'test3'], \n",
    "    'prediction': ['test1', 'test1', 'test3'], \n",
    "    'configuration': [False, False, False, False],\n",
    "    'acc': 0.22\n",
    "})\n",
    "\n",
    "\n",
    "df1 = pn.DataFrame(temp)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n    no_plant      0.919     0.962     0.940       343\n       plant      0.886     0.777     0.828       130\n\n    accuracy                          0.911       473\n   macro avg      0.903     0.870     0.884       473\nweighted avg      0.910     0.911     0.909       473\n\n"
     ]
    }
   ],
   "source": [
    "best = df[df['skip_n'] == 0].max()\n",
    "print(metrics.classification_report(best.ground_truth, best.prediction, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3953\n",
      "40\n",
      "40\n",
      "3953\n",
      "40\n",
      "40\n",
      "3953\n",
      "40\n",
      "40\n",
      "3953\n",
      "40\n",
      "40\n",
      "3953\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# CODE TO CONVERT MULTI-CLASS CLASSIFICATION INTO BINARY CLASSFICIATION\n",
    "# For DenseNet121\n",
    "#######################################################################\n",
    "\n",
    "for_skiped_layers = 6\n",
    "\n",
    "BINARY_PATH = 'binary-classification/'\n",
    "\n",
    "def convertStringToBoolArray(string: str):\n",
    "    if type(string) == list:\n",
    "        return string\n",
    "    temp = [item.replace('[', '').replace(']', '').strip() for item in string.split(' ')]\n",
    "    return [(x == 'True') for x in temp if len(x) == len('True') or len(x) == len('False')]\n",
    "\n",
    "def convertStringToStringArray(string: str):\n",
    "    return [\n",
    "            x.replace('[', '')\n",
    "            .replace(']', '')\n",
    "            .replace(\"'\", '')\n",
    "            .strip() \n",
    "            for x in string.split(', ')\n",
    "        ]\n",
    "\n",
    "def filterForBinaryClassification(classes_list):\n",
    "    plant_classes = [\n",
    "        'Tulipa gesneriana', 'angiosperm', \n",
    "        'aster', 'floret', 'flower cluster', \n",
    "        'gazania', 'grape', 'hepatica', \n",
    "        'Night-blooming cereus', 'prairie gentian', \n",
    "        'raspberry', 'wild calla']\n",
    "    temp_cls = []\n",
    "    for name in classes_list:\n",
    "        temp_cls.append('plant' if name in plant_classes else 'no_plant')\n",
    "    return temp_cls\n",
    "\n",
    "df = pn.read_csv(os.path.join('../reports', 'report-densenet121-run.csv'))\n",
    "\n",
    "# conversion\n",
    "df['ground_truth'] = df['ground_truth'].apply(lambda item: convertStringToStringArray(item))\n",
    "df['prediction'] = df['prediction'].apply(lambda item: convertStringToStringArray(item))\n",
    "\n",
    "# adaption for binary classification\n",
    "df['ground_truth'] = df['ground_truth'].apply(lambda item: filterForBinaryClassification(item))\n",
    "df['prediction'] = df['prediction'].apply(lambda item: filterForBinaryClassification(item))\n",
    "\n",
    "entry = df[df['skip_n'] == for_skiped_layers]\n",
    "grndT = entry.ground_truth.values[0]\n",
    "pred = entry.prediction.values[0]\n",
    "\n",
    "report_dict = metrics.classification_report(grndT, pred, digits=3, output_dict=True)\n",
    "logging.info(metrics.classification_report(grndT, pred, digits=3))\n",
    "\n",
    "temp_df = pn.DataFrame(report_dict).transpose()\n",
    "file_path = os.path.join(BINARY_PATH, f'{for_skiped_layers}-densenet121-classification-report.csv')\n",
    "\n",
    "if not os.path.exists(BINARY_PATH):\n",
    "    os.mkdir(BINARY_PATH)\n",
    "temp_df.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}